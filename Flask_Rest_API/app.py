from flask import Flask, request, jsonify
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel

app = Flask(__name__)

# ====================================
# ‚öôÔ∏è Kh·ªüi t·∫°o SparkSession v·ªõi GCS connector
# ====================================
spark = (
    SparkSession.builder
    .appName("ChurnPredictionAPI")
    # N·∫°p GCS connector jar
    .config("spark.jars", "/opt/spark/jars/gcs-connector.jar")
    # Cho ph√©p Spark ƒë·ªçc GCS
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
    # S·ª≠ d·ª•ng Application Default Credentials t·ª´ metadata server
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true")
    # ID c·ªßa project GCP ‚Äî thay b·∫±ng ƒë√∫ng project c·ªßa b·∫°n
    .config("spark.hadoop.fs.gs.project.id", "nt533q13-distributed-ml")
    .getOrCreate()
)

# ====================================
# üì¶ Load model t·ª´ GCS
# ====================================
MODEL_PATH = "gs://nt533q13-spark-data/models/telco_rf"

print("üîπ ƒêang load m√¥ h√¨nh...")
model = PipelineModel.load(MODEL_PATH)
print("‚úÖ Load m√¥ h√¨nh th√†nh c√¥ng!")

# ====================================
# üöÄ API predict
# ====================================
@app.route("/predict", methods=["POST"])
def predict():
    try:
        data = request.get_json()
        df = spark.createDataFrame([data])

        result = model.transform(df).collect()[0]
        return jsonify({
            "churn_prediction": int(result["prediction"]),
            "churn_probability": float(result["probability"][1])
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/health", methods=["GET"])
def health():
    return "OK", 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080)
