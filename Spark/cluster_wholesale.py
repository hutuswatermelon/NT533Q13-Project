from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import matplotlib.pyplot as plt

# 1ï¸âƒ£ Khá»Ÿi táº¡o Spark
spark = SparkSession.builder \
    .appName("WholesaleCustomersClustering") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# 2ï¸âƒ£ Äá»c dá»¯ liá»‡u
df = spark.read.option("header", True).option("inferSchema", True).csv("gs://nt533q13-spark-data/data/wholesale_customers.csv")
print("âœ… Dá»¯ liá»‡u Ä‘Ã£ Ä‘á»c:")
df.show(5)

# 3ï¸âƒ£ Kiá»ƒm tra cÃ¡c cá»™t cÃ³ sáºµn
print("ğŸ“‹ CÃ¡c cá»™t:", df.columns)

# 4ï¸âƒ£ Chá»n cÃ¡c cá»™t sá»‘ Ä‘á»ƒ phÃ¢n cá»¥m
numeric_cols = ['Fresh','Milk','Grocery','Frozen','Detergents_Paper','Delicassen']

# 5ï¸âƒ£ Gom cá»™t thÃ nh vector Ä‘áº·c trÆ°ng
assembler = VectorAssembler(inputCols=numeric_cols, outputCol="raw_features")
assembled = assembler.transform(df)

# 6ï¸âƒ£ Chuáº©n hÃ³a dá»¯ liá»‡u (Ä‘Æ°a cÃ¡c Ä‘áº·c trÆ°ng vá» cÃ¹ng thang Ä‘o)
scaler = StandardScaler(inputCol="raw_features", outputCol="features", withStd=True, withMean=True)
scaled_data = scaler.fit(assembled).transform(assembled)

# 7ï¸âƒ£ Huáº¥n luyá»‡n mÃ´ hÃ¬nh KMeans
k = 5  # báº¡n cÃ³ thá»ƒ thá»­ k=3-8 Ä‘á»ƒ so sÃ¡nh
kmeans = KMeans(k=k, seed=42, featuresCol="features")
model = kmeans.fit(scaled_data)

# 8ï¸âƒ£ Dá»± Ä‘oÃ¡n cá»¥m
predictions = model.transform(scaled_data)
predictions.select(numeric_cols + ["prediction"]).show(10, truncate=False)

# 9ï¸âƒ£ ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng cá»¥m
evaluator = ClusteringEvaluator(featuresCol="features")
silhouette = evaluator.evaluate(predictions)
print(f"\nğŸ“ˆ Silhouette Score (Euclidean distance) = {silhouette:.4f}")

# 1ï¸âƒ£0ï¸âƒ£ TÃ­nh trung bÃ¬nh theo cá»¥m Ä‘á»ƒ hiá»ƒu Ã½ nghÄ©a tá»«ng nhÃ³m
print("\nğŸ“Š Trung bÃ¬nh má»—i cá»¥m:")
predictions.groupBy("prediction").avg(*numeric_cols).show()

# 1ï¸âƒ£1ï¸âƒ£ LÆ°u mÃ´ hÃ¬nh
model.save("gs://nt533q13-spark-data/models/kmeans_wholesale")
print("âœ… MÃ´ hÃ¬nh KMeans Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i /models/kmeans_wholesale")

# 1ï¸âƒ£2ï¸âƒ£ Váº½ biá»ƒu Ä‘á»“ 2 chiá»u (Milk vs Grocery) Ä‘á»ƒ minh há»a
sample = predictions.select("Milk", "Grocery", "prediction").toPandas()

plt.figure(figsize=(7,5))
plt.scatter(sample["Milk"], sample["Grocery"], c=sample["prediction"], cmap="rainbow", alpha=0.6)
plt.xlabel("Milk (Annual Spending)")
plt.ylabel("Grocery (Annual Spending)")
plt.title(f"KMeans Clustering (k={k}) - Wholesale Customers")
plt.savefig("gs://nt533q13-spark-data/plots/wholesale_clusters.png")
print("ğŸ“Š Biá»ƒu Ä‘á»“ cá»¥m Ä‘Ã£ lÆ°u táº¡i: /data/wholesale_clusters.png")

spark.stop()
